{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from models.Embedding import Embedding\n",
    "from models.Alignment import Alignment\n",
    "from models.Blending import Blending\n",
    "\n",
    "from utils.drive import open_url\n",
    "from utils.shape_predictor import align_face\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_args = Namespace(unprocessed_dir = 'unprocessed', output_dir = 'input/face', output_size = 1024,\n",
    " cache_dir = 'cache', inter_method = 'bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shape Predictor\n"
     ]
    }
   ],
   "source": [
    "#Loading Shape Predictor for Alignment\n",
    "cache_dir = Path(align_args.cache_dir)\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_dir = Path(align_args.output_dir)\n",
    "output_dir.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "print(\"Downloading Shape Predictor\")\n",
    "f=open_url(\"https://drive.google.com/uc?id=1huhv8PYpNNKbGCLOaYUjOgR1pY5pmbJx\", cache_dir=cache_dir, return_path=True)\n",
    "predictor = dlib.shape_predictor(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2.jpeg: Number of faces detected: 1\n"
     ]
    }
   ],
   "source": [
    "for im in Path(align_args.unprocessed_dir).glob(\"*.*\"):\n",
    "    faces = align_face(str(im),predictor)\n",
    "\n",
    "    for i,face in enumerate(faces):\n",
    "        if(align_args.output_size):\n",
    "            factor = 1024//align_args.output_size\n",
    "            assert align_args.output_size*factor == 1024\n",
    "            face_tensor = torchvision.transforms.ToTensor()(face).unsqueeze(0).cuda()\n",
    "            face_tensor_lr = face_tensor[0].cpu().detach().clamp(0, 1)\n",
    "            face = torchvision.transforms.ToPILImage()(face_tensor_lr)\n",
    "            if factor != 1:\n",
    "                face = face.resize((align_args.output_size, align_args.output_size), Image.LANCZOS)\n",
    "        if len(faces) > 1:\n",
    "            face.save(Path(align_args.output_dir) / (im.stem+f\"_{i}.png\"))\n",
    "        else:\n",
    "            face.save(Path(align_args.output_dir) / (im.stem + f\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O arguments\n",
    "ref: str = f'{input(\"Enter Original Image\")}.png'\n",
    "tgt: str = f'{input(\"Enter Target Image\")}.png'\n",
    "\n",
    "args = Namespace(input_dir='input/face', output_dir='output', im_path1 = ref, im_path2=tgt, im_path3= tgt,\n",
    " sign='fidelity', smooth=5, size=1024, ckpt='pretrained_models/ffhq.pt', channel_multiplier=2,\n",
    "  latent=512, n_mlp=8, device='cuda', seed=None, tile_latent=False, opt_name='adam',\n",
    "   learning_rate=0.01, lr_schedule='fixed', save_intermediate=False, save_interval=300,\n",
    "    verbose=False, seg_ckpt='pretrained_models/seg.pth', percept_lambda=1.0, l2_lambda=1.0,\n",
    "     p_norm_lambda=0.001, l_F_lambda=0.1, W_steps=250, FS_steps=250, ce_lambda=1.0, style_lambda=40000.0,\n",
    "      align_steps1=100, align_steps2=100, face_lambda=1.0, hair_lambda=1.0, blend_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path1 = os.path.join(args.input_dir, args.im_path1)\n",
    "im_path2 = os.path.join(args.input_dir, args.im_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN2 from checkpoint: pretrained_models/ffhq.pt\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /mnt/d/Programming/Spectrum/Barbershop/Barbershop/losses/lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n",
      "Number of images: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:54<00:00, 54.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Images: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:48<00:00, 48.99s/it]\n"
     ]
    }
   ],
   "source": [
    "#Embed input image\n",
    "ii2s = Embedding(args)\n",
    "\n",
    "im_set = {im_path1}\n",
    "ii2s.invert_images_in_W([*im_set])\n",
    "ii2s.invert_images_in_FS([*im_set])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN2 from checkpoint: pretrained_models/ffhq.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "#Alignment Step\n",
    "\n",
    "align = Alignment(args)\n",
    "align.align_images(im_path1, im_path2, sign=args.sign, align_more_region=False, smooth=args.smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN2 from checkpoint: pretrained_models/ffhq.pt\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /mnt/d/Programming/Spectrum/Barbershop/Barbershop/losses/masked_lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /mnt/d/Programming/Spectrum/Barbershop/Barbershop/losses/masked_lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "blend = Blending(args)\n",
    "blend.blend_images(im_path1, im_path2, im_path2, sign=args.sign)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
